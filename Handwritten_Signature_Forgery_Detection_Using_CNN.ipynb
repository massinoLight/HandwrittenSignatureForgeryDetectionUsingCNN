{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten_Signature_Forgery_Detection_Using_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsVsNFIY6IlOQRQ4BTjHPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massinoLight/HandwrittenSignatureForgeryDetectionUsingCNN/blob/master/Handwritten_Signature_Forgery_Detection_Using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpxvLqCJUGCm",
        "outputId": "3bd3799c-c172-4598-ed60-1f02f8584b27"
      },
      "source": [
        "#Import une partie de la dataset du fichier zip qui est sur le github\n",
        "import pathlib\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "#Import dataset\n",
        "import pathlib\n",
        "import os\n",
        "data_dir = tf.keras.utils.get_file(\n",
        "    \"datasets.zip\",\n",
        "    \"https://github.com/massinoLight/HandwrittenSignatureForgeryDetectionUsingCNN/raw/master/datasets.zip\",\n",
        "    extract=False)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(data_dir, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/datasets')\n",
        "\n",
        "data_dir = pathlib.Path('/content/datasets/datasets')\n",
        "print(data_dir)\n",
        "print(os.path.abspath(data_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/datasets/datasets\n",
            "/content/datasets/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-LzkxOBVO3k"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKq6GOYQVNEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61SIk96vUblv",
        "outputId": "15ee7c2b-cb60-47d6-ec7c-a85ff40ed988"
      },
      "source": [
        "image_count = len(list(data_dir.glob('*/*')))\n",
        "print(image_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yku_FXAvVQOo",
        "outputId": "a5bf2241-b7e8-4bb3-83cf-81652764167a"
      },
      "source": [
        "batch_size = 3\n",
        "img_height = 200\n",
        "img_width = 200\n",
        "\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=42,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  )\n",
        "\n",
        "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=42,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = val_data.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18 files belonging to 2 classes.\n",
            "Using 15 files for training.\n",
            "Found 18 files belonging to 2 classes.\n",
            "Using 3 files for validation.\n",
            "['fake', 'true']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}