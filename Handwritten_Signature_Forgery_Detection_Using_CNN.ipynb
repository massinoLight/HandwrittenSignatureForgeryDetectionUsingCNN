{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handwritten_Signature_Forgery_Detection_Using_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0ZJP7XNJsh7Ci0JirfBg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massinoLight/HandwrittenSignatureForgeryDetectionUsingCNN/blob/master/Handwritten_Signature_Forgery_Detection_Using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcq4OBncL-8d"
      },
      "source": [
        "**Importer les data sets presentes sur github au format zip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpxvLqCJUGCm",
        "outputId": "3bd3799c-c172-4598-ed60-1f02f8584b27"
      },
      "source": [
        "#Import une partie de la dataset du fichier zip qui est sur le github\n",
        "import pathlib\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "import datetime\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "#Import dataset\n",
        "import pathlib\n",
        "import os\n",
        "data_dir = tf.keras.utils.get_file(\n",
        "    \"datasets.zip\",\n",
        "    \"https://github.com/massinoLight/HandwrittenSignatureForgeryDetectionUsingCNN/raw/master/datasets.zip\",\n",
        "    extract=False)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(data_dir, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/datasets')\n",
        "\n",
        "data_dir = pathlib.Path('/content/datasets/datasets')\n",
        "print(data_dir)\n",
        "print(os.path.abspath(data_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/datasets/datasets\n",
            "/content/datasets/datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKq6GOYQVNEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61SIk96vUblv",
        "outputId": "15ee7c2b-cb60-47d6-ec7c-a85ff40ed988"
      },
      "source": [
        "image_count = len(list(data_dir.glob('*/*')))\n",
        "print(image_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIr3QoZ-MbyY"
      },
      "source": [
        "**Préprocess les images avec une fonction de keras afin de les avoir de la meme taille**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yku_FXAvVQOo",
        "outputId": "a5bf2241-b7e8-4bb3-83cf-81652764167a"
      },
      "source": [
        "#le nombre d'image a envoyer a notre reseau ici 3x3 (car nous ne disposons pas encore d'énormement d'images)\n",
        "batch_size = 3\n",
        "img_height = 200\n",
        "img_width = 200\n",
        "\n",
        "#recup les images (d'entrainement) dans le repértoire chaque nom de répertoire= une classe (ici uniquement deux ma vrai signature et des fausses)\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=42,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  )\n",
        "#recup les images (de validation) (ici je garde le méme répértoire pour les données d'entrainement et les données de test mais a changer par la suite)\n",
        "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=42,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = val_data.class_names\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18 files belonging to 2 classes.\n",
            "Using 15 files for training.\n",
            "Found 18 files belonging to 2 classes.\n",
            "Using 3 files for validation.\n",
            "['fake', 'true']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxThmrhRVNZ"
      },
      "source": [
        "![relu.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ksxIEFZL3CA"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "#le on définie le nombre de classes ici nous disposons de 2 true et fake\n",
        "num_classes = 2\n",
        "\n",
        "#on va définir notre réseau de neuron (ici avec 4 couches )\n",
        "model = tf.keras.Sequential([\n",
        "    #on normalise dans un premier temps notre jeux de données\n",
        "    layers.experimental.preprocessing.Rescaling(1./255),\n",
        "    #on définie nos couches de convolution (128 neurons avec une fonction d'activation relu (Unité de rectification linéaire) ) et de pooling (avec un max pooling) \n",
        "    layers.Conv2D(128,4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64,4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32,4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(16,4, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64,activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'],)\n",
        "\n",
        "logdir=\"logs\"\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1, write_images=logdir,\n",
        "                                                   embeddings_data=train_data)\n",
        "\n",
        "model.fit( \n",
        "    train_data,\n",
        "  validation_data=val_data,\n",
        "  epochs=2,\n",
        "  callbacks=[tensorboard_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}